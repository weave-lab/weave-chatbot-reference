{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc942c",
   "metadata": {},
   "source": [
    "# LLM Evaluations for RAG Systems\n",
    "\n",
    "Given the stochastic nature of Large Language Models (LLMs), establishing robust evaluation criteria is crucial for building confidence in their performance. For Retrieval-Augmented Generation (RAG) systems, comprehensive evaluation requires assessing both the retrieval and generation components to ensure system reliability and accuracy.\n",
    "\n",
    "## Background\n",
    "\n",
    "In the 101 RAG Hands-On Training, we demonstrated how LLM judges can be utilized to evaluate RAG systems effectively.\n",
    "\n",
    "- **[Evaluation Documentation Reference](https://docs.google.com/document/d/1Rg1QXZ5Cg0aX8hYvRrvevY1uz6lPpZkaasoqW7Pcm9o/edit?tab=t.0#heading=h.jjijsv4v12qe)** \n",
    "- **[Evaluation Code Reference](./../../workshop-101/eval_rag.py)** \n",
    "\n",
    "## Workshop Objectives\n",
    "\n",
    "In this notebook, we will explore advanced evaluation techniques using **[Ragas](https://github.com/explodinggradients/ragas)**. It will help you implement systematic evaluation workflows to measure and improve your RAG system's performance across various metrics and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58e64c",
   "metadata": {},
   "source": [
    "## Ragas\n",
    "\n",
    "Ragas is an open-source library published under the Apache 2.0 license that provides a comprehensive toolkit for evaluating and optimizing LLM applications. It offers specialized metrics and evaluation frameworks, making it easier to assess LLM generations.\n",
    "\n",
    "### Installation\n",
    "\n",
    "You can install Ragas using UV (our preferred package manager):\n",
    "\n",
    "```bash\n",
    "uv add ragas\n",
    "```\n",
    "\n",
    "Alternatively, you can install it with pip:\n",
    "\n",
    "```bash\n",
    "pip install ragas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69fd6d",
   "metadata": {},
   "source": [
    "### Setting up Ragas\n",
    "\n",
    "Install the LangChain wrapper for Vertex AI to use Vertex AI models in Ragas:\n",
    "\n",
    "```bash\n",
    "uv add langchain-google-vertexai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "# Define global constants for project and location\n",
    "PROJECT_ID = \"weave-ai-sandbox\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "# LangChain wrapper is required to use Vertex AI models with RAGAS\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatVertexAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d616f",
   "metadata": {},
   "source": [
    "### Retriever Evaluation \n",
    "\n",
    "In the 101 workshop, we demonstrated how the retrieval system's ability to rank relevant chunks can be evaluated using context precision. Context Precision is a metric that evaluates the retriever's ability to rank relevant chunks higher than irrelevant ones for a given query in the retrieved context. Specifically, it assesses the degree to which relevant chunks in the retrieved context are placed at the top of the ranking.\n",
    "\n",
    "Before implementing the code, take a moment to review the Ragas documentation to understand how they calculate context precision. Also experiment with calculating Context Recall, which measures how many of the relevant documents were successfully retrieved.\n",
    "\n",
    "**References:**\n",
    "- Code reference to base implementation: [Base implementation](./../../workshop-101/eval_rag.py#115)\n",
    "- Ragas documentation: [Context Precision metric](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/)\n",
    "- Ragas documentation: [Context Recall metric](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_recall/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2361576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "# Example usage when you do not have a reference response\n",
    "context_precision_without_reference = LLMContextPrecisionWithoutReference(\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "sample_without_reference = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"The Eiffel Tower is located in Paris.\",  # Experiment with changing the response here\n",
    "    retrieved_contexts=[\n",
    "        \"The Eiffel Tower is located in Paris.\"\n",
    "    ],  # Experiment with adding irrelevant contexts here\n",
    ")\n",
    "\n",
    "await context_precision_without_reference.single_turn_ascore(sample_without_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextPrecisionWithReference\n",
    "\n",
    "# Example usage when you have a reference response\n",
    "context_precision_with_reference = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
    "sample_with_reference = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\",\n",
    "    retrieved_contexts=[\n",
    "        \"The Eiffel Tower is located in Paris.\"\n",
    "    ],  # Experiment with adding irrelevant contexts here\n",
    ")\n",
    "\n",
    "await context_precision_with_reference.single_turn_ascore(sample_with_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall\n",
    "\n",
    "# Higher recall means fewer relevant documents were left out.\n",
    "context_recall = LLMContextRecall(llm=evaluator_llm)\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\",\n",
    "    retrieved_contexts=[\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"The Eiffel Tower is located in Paris.\",  # Experiment by removing this relevant context\n",
    "    ],\n",
    ")\n",
    "\n",
    "await context_recall.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9699c4",
   "metadata": {},
   "source": [
    "The above metrics use LLMs to judge the retrieved context, but you can also use non-LLM judge metrics if you know the exact relevance of each context. You can look into Non-LLM Based metrics in the Ragas documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44bfc2",
   "metadata": {},
   "source": [
    "### Generation Evaluation \n",
    "\n",
    "After ensuring the retrieval context is relevant and all the relevant context is being retrieved, we need to evaluate the response quality of the RAG system.\n",
    "\n",
    "In the 101 RAG Hands-On Training, we also demonstrated how LLM judge can be utilized to evaluate answer quality.\n",
    "\n",
    "**References:**\n",
    "- Code reference to answer quality evaluation: [Answer Quality](./../../workshop-101/eval_rag.py#44)\n",
    "- Ragas documentation: [Faithfulness Metric](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import Faithfulness\n",
    "\n",
    "# Faithfulness metric measures how factually consistent a response is with the retrieved context.\n",
    "# It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where and when was Einstein born?\",\n",
    "    response=\"Einstein was born in Germany on 14th March 1879.\",  # Experiment with changing the response here\n",
    "    retrieved_contexts=[\n",
    "        \"Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\"\n",
    "    ],\n",
    ")\n",
    "scorer = Faithfulness(llm=evaluator_llm)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2f8d8",
   "metadata": {},
   "source": [
    "### Aspect Critique\n",
    "\n",
    "Ragas has a evaluation concept called Aspect Critique that is designed to assess submissions based on predefined or custom aspects. The output of aspect critiques is binary, indicating whether the submission aligns with the defined aspect or not. This evaluation is performed using the 'answer' as input.\n",
    "\n",
    "**References:**\n",
    "- Ragas documentation: [Aspect Critique](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/aspect_critic/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa63cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"The Eiffel Tower is located in Paris.\",\n",
    "    reference=\"The Eiffel Tower is located in Paris.\",\n",
    ")\n",
    "\n",
    "scorer = AspectCritic(\n",
    "    name=\"correctness\",\n",
    "    definition=\"Does the response accurately answer the user's question?\",\n",
    ")\n",
    "scorer.llm = evaluator_llm\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17114492",
   "metadata": {},
   "source": [
    "#### Integration with the RAG Chatbot System\n",
    "\n",
    "Now we'll integrate the **RAG Pipeline** from `app_201.py` to evaluate the RAG system instead of sample data.\n",
    "\n",
    "**Key Components from app_201.py:**\n",
    "- **`generate_chat_response()`**: The main function that generates responses using retrieved context\n",
    "- **`read_prompt_from_file()`**: Loads the system prompt that guides response generation\n",
    "- **System prompts**: The actual prompts used in production\n",
    "- **Chat history handling**: Maintains conversation context\n",
    "- **GenAI client configuration**: Same model and parameters as production\n",
    "\n",
    "This allows you to evaluate your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fa9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the complete RAG system from app_201.py\n",
    "from app_201 import generate_chat_response, read_prompt_from_file, init_vector_store\n",
    "from vector_store import MilvusVectorStore\n",
    "from google import genai\n",
    "\n",
    "# Initialize GenAI Client\n",
    "genai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# NOTE: You cannot simultaneously run the chatbot app and this evaluation notebook\n",
    "vector_store = init_vector_store(\n",
    "    genai_client, collection_name=\"weave_docs\", reingest=False\n",
    ")  # Set reingest to True if you want to reingest the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for systematic RAG evaluation using the complete app_201 pipeline\n",
    "def create_evaluation_sample_with_rag(\n",
    "    user_input: str,\n",
    "    vector_store: MilvusVectorStore,\n",
    "    genai_client,\n",
    "    rag_llm_model: str = \"gemini-2.5-flash\",\n",
    "    collection_name: str = \"weave_docs\",\n",
    "    top_k: int = 5,\n",
    "    system_prompt_version: str = \"v1\",\n",
    "    chat_history: list = None,\n",
    ") -> SingleTurnSample:\n",
    "    \"\"\"\n",
    "    Create a SingleTurnSample using the complete RAG pipeline from app_201.py.\n",
    "\n",
    "    This function:\n",
    "    1. Retrieves relevant contexts from vector store\n",
    "    2. Generates an actual response using the RAG system\n",
    "    3. Returns both for evaluation\n",
    "\n",
    "    Args:\n",
    "        user_input: The user's question/query\n",
    "        vector_store: The initialized MilvusVectorStore instance\n",
    "        genai_client: The GenAI client for response generation\n",
    "        rag_llm_model: The LLM model to use for RAG response generation\n",
    "        collection_name: Name of the collection to retrieve from\n",
    "        top_k: Number of top similar documents to retrieve\n",
    "        system_prompt_version: Version of system prompt to use\n",
    "        chat_history: Previous conversation history\n",
    "\n",
    "    Returns:\n",
    "        SingleTurnSample with real RAG-generated response\n",
    "    \"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    # Step 1: Retrieve relevant contexts (same as before)\n",
    "    retrieved_contexts = vector_store.retrieve(\n",
    "        query=user_input, collection_name=collection_name, top_k=top_k, verbose=False\n",
    "    )\n",
    "\n",
    "    # Step 2: Generate actual response using the RAG system from app_201.py\n",
    "    system_prompt = read_prompt_from_file(system_prompt_version)\n",
    "\n",
    "    response = generate_chat_response(\n",
    "        client=genai_client,\n",
    "        system_prompt=system_prompt,\n",
    "        user_message=user_input,\n",
    "        chat_history=chat_history,\n",
    "        context_snippets=retrieved_contexts,\n",
    "        model=rag_llm_model,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return SingleTurnSample(\n",
    "        user_input=user_input,\n",
    "        response=response,\n",
    "        retrieved_contexts=retrieved_contexts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab538761",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_LLM_MODEL = \"gemini-2.5-flash-lite\"\n",
    "VECTOR_TOP_K = 5\n",
    "\n",
    "# Test query related to WAML documentation\n",
    "user_query = \"What is the WAML file and where does it live?\"\n",
    "\n",
    "# Create sample using the complete RAG pipeline\n",
    "rag_sample = create_evaluation_sample_with_rag(\n",
    "    user_input=user_query,\n",
    "    vector_store=vector_store,\n",
    "    genai_client=genai_client,\n",
    "    rag_llm_model=RAG_LLM_MODEL,\n",
    "    collection_name=\"weave_docs\",\n",
    "    top_k=VECTOR_TOP_K,\n",
    "    system_prompt_version=\"v1\",\n",
    ")\n",
    "\n",
    "print(f\"User Query: {rag_sample.user_input}\")\n",
    "print(f\"RAG Generated Response: {rag_sample.response}\")\n",
    "print(f\"Retrieved {len(rag_sample.retrieved_contexts)} contexts\")\n",
    "\n",
    "# Evaluate context precision for the RAG-generated response\n",
    "rag_context_precision_score = (\n",
    "    await context_precision_without_reference.single_turn_ascore(rag_sample)\n",
    ")\n",
    "print(f\"Context Precision Score (RAG): {rag_context_precision_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch evaluation using the complete RAG system\n",
    "test_queries = [\n",
    "    \"What is the purpose of the slug field in WAML?\",\n",
    "    \"How do you configure Slack notifications in WAML?\",\n",
    "    \"What is the deploy section used for in WAML?\",\n",
    "    \"How do you specify the owner of a repository in WAML?\",\n",
    "    \"What are external links used for in WAML?\",\n",
    "]\n",
    "\n",
    "print(\"Creating evaluation samples using the complete RAG pipeline...\")\n",
    "evaluation_samples = []\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"Generating response for query {i + 1}: {query[:50]}...\")\n",
    "\n",
    "    # Generate actual responses using the RAG system\n",
    "    sample = create_evaluation_sample_with_rag(\n",
    "        user_input=query,\n",
    "        vector_store=vector_store,\n",
    "        genai_client=genai_client,\n",
    "        rag_llm_model=RAG_LLM_MODEL,\n",
    "        collection_name=\"weave_docs\",\n",
    "        top_k=VECTOR_TOP_K,\n",
    "        system_prompt_version=\"v1\",\n",
    "    )\n",
    "\n",
    "    evaluation_samples.append(sample)\n",
    "    print(f\"✓ Generated response: {sample.response[:10]}...\")\n",
    "\n",
    "print(\n",
    "    f\"\\nCreated {len(evaluation_samples)} evaluation samples with REAL RAG responses!\"\n",
    ")\n",
    "print(\"Each sample contains:\")\n",
    "print(\"  - User query\")\n",
    "print(\"  - RAG-generated response (using app_201.py pipeline)\")\n",
    "print(\"  - Retrieved contexts from vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def evaluate_sample(sample):\n",
    "    \"\"\"Evaluate a single sample and return the score.\"\"\"\n",
    "    return await context_precision_without_reference.single_turn_ascore(sample)\n",
    "\n",
    "\n",
    "# Parallel execution of evaluations\n",
    "evaluation_tasks = [evaluate_sample(sample) for sample in evaluation_samples]\n",
    "scores = await asyncio.gather(*evaluation_tasks)\n",
    "\n",
    "avg_score = statistics.mean(scores)\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Average Context Precision: {avg_score:.3f}\")\n",
    "print(f\"Min Score: {min(scores):.3f}\")\n",
    "print(f\"Max Score: {max(scores):.3f}\")\n",
    "print(f\"Standard Deviation: {statistics.stdev(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8e58a",
   "metadata": {},
   "source": [
    "## Next Steps and Experimentation\n",
    "\n",
    "Congratulations! You've successfully implemented RAG system evaluation using Ragas. Now it's time to experiment and further optimize your system.\n",
    "\n",
    "### Suggested Experiments\n",
    "\n",
    "#### 1. **Expand Your Data Sources**\n",
    "- Add more documents to your vector store beyond the current WAML documentation.\n",
    "- Try different document types (e.g., web pages, structured data).\n",
    "\n",
    "#### 2. **Optimize Chunking Strategy**\n",
    "- Experiment with different chunk sizes in `chunking.py`.\n",
    "- Try different chunking strategies. Refer to the [Pinecone Reference](https://www.pinecone.io/learn/chunking-strategies/).\n",
    "- Measure how chunking affects both retrieval and generation quality.\n",
    "\n",
    "#### 3. **Comprehensive Metric Evaluation**\n",
    "- Implement additional Ragas metrics.\n",
    "- Create reference answers for your test queries to enable metrics that require ground truth.\n",
    "\n",
    "#### 4. **Prompt Engineering Optimization**\n",
    "- Experiment with different system prompts.\n",
    "- Optimize prompts based on evaluation results.\n",
    "- Test how different prompts affect metric scores.\n",
    "\n",
    "### ⚠️ **Cost Considerations**\n",
    "\n",
    "**Important Warning**: All the evaluation metrics in this notebook use LLMs (Gemini models) as judges, which incur API costs. Running evaluations repeatedly can lead to significant expenses.\n",
    "\n",
    "**Cost Management Tips**:\n",
    "- Start with small test sets (5–10 samples) before scaling up.\n",
    "- Avoid running evaluations repeatedly on the same configuration.\n",
    "- Test the system qualitatively on a few samples to ensure it is improving before running the full evaluation suite.\n",
    "\n",
    "### Google GenAI Evaluation \n",
    "\n",
    "Google also provides an evaluation suite for LLM generations. Refer to [Gen AI Evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
