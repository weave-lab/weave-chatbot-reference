{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc942c",
   "metadata": {},
   "source": [
    "# LLM Evaluations for RAG Systems\n",
    "\n",
    "Given the stochastic nature of Large Language Models (LLMs), establishing robust evaluation criteria is crucial for building confidence in their performance. For Retrieval-Augmented Generation (RAG) systems, comprehensive evaluation requires assessing both the retrieval and generation components to ensure system reliability and accuracy.\n",
    "\n",
    "## Background\n",
    "\n",
    "In the 101 RAG Hands-On Training, we demonstrated how LLM Judges can be utilized to evaluate RAG systems effectively. \n",
    "\n",
    "- **[Evaluation Documentation Reference](https://docs.google.com/document/d/1Rg1QXZ5Cg0aX8hYvRrvevY1uz6lPpZkaasoqW7Pcm9o/edit?tab=t.0#heading=h.jjijsv4v12qe)** \n",
    "- **[Evaluation Code Reference](./../workshop-101/eval_rag.py)** \n",
    "\n",
    "## Workshop Objectives\n",
    "\n",
    "In this notebook, we will explore advanced evaluation techniques using two powerful libraries:\n",
    "- **[Ragas](https://github.com/explodinggradients/ragas)** \n",
    "- **[Google Gen AI Evaluation Service](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview)** \n",
    "\n",
    "These tools will help you implement systematic evaluation workflows to measure and improve your RAG system's performance across various metrics and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58e64c",
   "metadata": {},
   "source": [
    "## Ragas\n",
    "\n",
    "Ragas is an open-source library published under the Apache 2.0 license that provides a comprehensive toolkit for evaluating and optimizing LLM applications. It offers specialized metrics and evaluation frameworks making it easier to assess LLM generations\n",
    "\n",
    "### Installation\n",
    "\n",
    "You can install Ragas using UV (our preferred package manager):\n",
    "\n",
    "```bash\n",
    "uv add ragas\n",
    "```\n",
    "\n",
    "Alternatively, you can install it with pip:\n",
    "\n",
    "```bash\n",
    "pip install ragas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69fd6d",
   "metadata": {},
   "source": [
    "### Setting up Ragas\n",
    "\n",
    "Install the Langchain wrapper for Vertex AI to use Vertex AI models in Ragas:\n",
    "\n",
    "```bash\n",
    "uv add langchain-google-vertexai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "# Define global constants for project and location\n",
    "PROJECT_ID = \"weave-ai-sandbox\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatVertexAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2352c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional modules for vector store integration\n",
    "from pathlib import Path\n",
    "from google import genai\n",
    "from vector_store import MilvusVectorStore\n",
    "\n",
    "# Initialize GenAI Client for vector store operations\n",
    "genai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fa9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the complete RAG system from app_201.py\n",
    "from app_201 import generate_chat_response, read_prompt_from_file\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "# Initialize the context precision metric\n",
    "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for systematic RAG evaluation using the complete app_201 pipeline\n",
    "\n",
    "\n",
    "def create_evaluation_sample_with_rag(\n",
    "    user_input: str,\n",
    "    vector_store: MilvusVectorStore,\n",
    "    genai_client,\n",
    "    collection_name: str = \"weave_docs\",\n",
    "    top_k: int = 5,\n",
    "    system_prompt_version: str = \"v1\",\n",
    "    chat_history: list = None,\n",
    ") -> SingleTurnSample:\n",
    "    \"\"\"\n",
    "    Create a SingleTurnSample using the complete RAG pipeline from app_201.py.\n",
    "\n",
    "    This function:\n",
    "    1. Retrieves relevant contexts from vector store\n",
    "    2. Generates an actual response using the RAG system\n",
    "    3. Returns both for evaluation\n",
    "\n",
    "    Args:\n",
    "        user_input: The user's question/query\n",
    "        vector_store: The initialized MilvusVectorStore instance\n",
    "        genai_client: The GenAI client for response generation\n",
    "        collection_name: Name of the collection to retrieve from\n",
    "        top_k: Number of top similar documents to retrieve\n",
    "        system_prompt_version: Version of system prompt to use\n",
    "        chat_history: Previous conversation history\n",
    "\n",
    "    Returns:\n",
    "        SingleTurnSample with real RAG-generated response\n",
    "    \"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    # Step 1: Retrieve relevant contexts (same as before)\n",
    "    retrieved_contexts = vector_store.retrieve(\n",
    "        query=user_input, collection_name=collection_name, top_k=top_k, verbose=False\n",
    "    )\n",
    "\n",
    "    # Step 2: Generate actual response using the RAG system from app_201.py\n",
    "    system_prompt = read_prompt_from_file(system_prompt_version)\n",
    "\n",
    "    response = generate_chat_response(\n",
    "        client=genai_client,\n",
    "        system_prompt=system_prompt,\n",
    "        user_message=user_input,\n",
    "        chat_history=chat_history,\n",
    "        context_snippets=retrieved_contexts,\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return SingleTurnSample(\n",
    "        user_input=user_input,\n",
    "        response=response,  # This is now a REAL response from the RAG system\n",
    "        retrieved_contexts=retrieved_contexts,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_evaluation_sample_mock_response(\n",
    "    user_input: str,\n",
    "    response: str,\n",
    "    vector_store: MilvusVectorStore,\n",
    "    collection_name: str = \"weave_docs\",\n",
    "    top_k: int = 5,\n",
    ") -> SingleTurnSample:\n",
    "    \"\"\"\n",
    "    Create a SingleTurnSample for evaluation using a mock response (original approach).\n",
    "\n",
    "    Use this when you want to test against known/expected responses.\n",
    "    \"\"\"\n",
    "    retrieved_contexts = vector_store.retrieve(\n",
    "        query=user_input, collection_name=collection_name, top_k=top_k, verbose=False\n",
    "    )\n",
    "\n",
    "    return SingleTurnSample(\n",
    "        user_input=user_input,\n",
    "        response=response,\n",
    "        retrieved_contexts=retrieved_contexts,\n",
    "    )\n",
    "\n",
    "\n",
    "async def evaluate_context_precision(\n",
    "    sample: SingleTurnSample, metric: LLMContextPrecisionWithoutReference\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate context precision for a given sample.\n",
    "\n",
    "    Args:\n",
    "        sample: The SingleTurnSample to evaluate\n",
    "        metric: The context precision metric instance\n",
    "\n",
    "    Returns:\n",
    "        The context precision score\n",
    "    \"\"\"\n",
    "    return await metric.single_turn_ascore(sample)\n",
    "\n",
    "\n",
    "async def batch_evaluate_samples(\n",
    "    samples: list[SingleTurnSample], metric: LLMContextPrecisionWithoutReference\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple samples and return their scores.\n",
    "\n",
    "    Args:\n",
    "        samples: List of SingleTurnSample instances\n",
    "        metric: The context precision metric instance\n",
    "\n",
    "    Returns:\n",
    "        List of context precision scores\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        score = await evaluate_context_precision(sample, metric)\n",
    "        scores.append(score)\n",
    "        print(f\"Sample {i + 1}: Context Precision = {score:.3f}\")\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"Enhanced evaluation utility functions defined successfully!\")\n",
    "print(\"✓ create_evaluation_sample_with_rag() - Uses complete RAG pipeline\")\n",
    "print(\"✓ create_evaluation_sample_mock_response() - Uses mock responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a808b",
   "metadata": {},
   "source": [
    "### Integration with app_201.py RAG System\n",
    "\n",
    "Now we'll integrate the **complete RAG pipeline** from `app_201.py` to evaluate the actual system responses rather than mock responses.\n",
    "\n",
    "**Key Components from app_201.py:**\n",
    "- **`generate_chat_response()`**: The main function that generates responses using retrieved context\n",
    "- **`read_prompt_from_file()`**: Loads the system prompt that guides response generation\n",
    "- **System prompts**: The actual prompts used in production (v1, v2, v3)\n",
    "- **Chat history handling**: Maintains conversation context\n",
    "- **GenAI client configuration**: Same model and parameters as production\n",
    "\n",
    "**Two Evaluation Approaches:**\n",
    "1. **Real RAG Evaluation**: Uses the complete pipeline to generate actual responses\n",
    "2. **Mock Response Evaluation**: Tests against known/expected responses (useful for regression testing)\n",
    "\n",
    "This allows you to evaluate both the retrieval quality AND the generation quality of your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Milvus vector store (same logic as app_201.py)\n",
    "def init_vector_store(\n",
    "    client: genai.Client, collection_name: str = \"weave_docs\", reingest: bool = False\n",
    ") -> MilvusVectorStore:\n",
    "    \"\"\"Initialize the Milvus vector store and ingest documents if needed.\"\"\"\n",
    "    current_file = Path.cwd()  # Using current working directory for notebook context\n",
    "    doc_paths = [str(current_file / \"data\" / \"waml.md\")]\n",
    "    vector_db_path = current_file / \"vector_db\" / \"milvus.db\"\n",
    "\n",
    "    # Ensure the parent directory exists\n",
    "    vector_db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Initialize Milvus vector store\n",
    "    vector_store = MilvusVectorStore(\n",
    "        vector_db_path=str(vector_db_path), genai_client=client\n",
    "    )\n",
    "\n",
    "    # Create collection if it doesn't exist or if reingestion is forced\n",
    "    if reingest or not vector_store.milvus_client.has_collection(collection_name):\n",
    "        vector_store.create_collection(doc_paths, collection_name=collection_name)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# Initialize the vector store\n",
    "print(\"Initializing Milvus vector store...\")\n",
    "vector_store = init_vector_store(\n",
    "    genai_client, collection_name=\"weave_docs\", reingest=False\n",
    ")\n",
    "print(\"Vector store initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041538e",
   "metadata": {},
   "source": [
    "### Retriever Evaluation \n",
    "\n",
    "In the 101 workshop, we demonstrated how the retrieval system's ability to rank relevant chunks can be evaluated using context precision. This evaluation was based on the Ragas metric called Context Precision.\n",
    "\n",
    "**References:**\n",
    "- Code reference to base implementation: [Base implementation](./../workshop-101/eval_rag.py#115)\n",
    "- Ragas documentation: [Context Precision metric](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/)\n",
    "\n",
    "Before implementing the code, take a moment to go through the Ragas documentation to understand how they calculate context precision. \n",
    "\n",
    "Now, let's implement the Ragas version of this metric to evaluate retrieval performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab538761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Using the complete RAG pipeline from app_201.py\n",
    "VECTOR_TOP_K = 5  # Number of similar documents to retrieve\n",
    "\n",
    "# Test query related to WAML documentation\n",
    "user_query = \"What is the WAML file and where does it live?\"\n",
    "\n",
    "print(\"=== Evaluating with REAL RAG System (app_201.py pipeline) ===\")\n",
    "\n",
    "# Create sample using the complete RAG pipeline\n",
    "rag_sample = create_evaluation_sample_with_rag(\n",
    "    user_input=user_query,\n",
    "    vector_store=vector_store,\n",
    "    genai_client=genai_client,\n",
    "    collection_name=\"weave_docs\",\n",
    "    top_k=VECTOR_TOP_K,\n",
    "    system_prompt_version=\"v1\",\n",
    ")\n",
    "\n",
    "print(f\"User Query: {rag_sample.user_input}\")\n",
    "print(f\"RAG Generated Response: {rag_sample.response}\")\n",
    "print(f\"Retrieved {len(rag_sample.retrieved_contexts)} contexts\")\n",
    "\n",
    "# Evaluate context precision for the RAG-generated response\n",
    "rag_context_precision_score = await context_precision.single_turn_ascore(rag_sample)\n",
    "print(f\"Context Precision Score (RAG): {rag_context_precision_score:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"=== Comparison: Mock Response vs RAG Response ===\")\n",
    "\n",
    "# Mock response for comparison\n",
    "mock_response = \"\"\"The WAML™ is the `.weave.yaml` file that lives in the root of all service repos. It defines how a service is deployed and contains configuration details like the friendly name, GitHub slug, owner, Slack channel, namespace, and deployment specifications.\"\"\"\n",
    "\n",
    "# Create sample with mock response\n",
    "mock_sample = create_evaluation_sample_mock_response(\n",
    "    user_input=user_query,\n",
    "    response=mock_response,\n",
    "    vector_store=vector_store,\n",
    "    collection_name=\"weave_docs\",\n",
    "    top_k=VECTOR_TOP_K,\n",
    ")\n",
    "\n",
    "# Evaluate context precision for the mock response\n",
    "mock_context_precision_score = await context_precision.single_turn_ascore(mock_sample)\n",
    "\n",
    "print(f\"Context Precision Score (Mock): {mock_context_precision_score:.3f}\")\n",
    "print(f\"Context Precision Score (RAG):  {rag_context_precision_score:.3f}\")\n",
    "print(f\"Difference: {rag_context_precision_score - mock_context_precision_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09471a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Utility functions have been moved earlier in the notebook\n",
    "# This cell can be used for additional evaluation utilities or removed\n",
    "\n",
    "print(\"All utility functions are now defined earlier in the notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch evaluation using the complete RAG system\n",
    "\n",
    "# Define test queries (no need for expected responses since we'll generate them)\n",
    "test_queries = [\n",
    "    \"What is the purpose of the slug field in WAML?\",\n",
    "    \"How do you configure Slack notifications in WAML?\",\n",
    "    \"What is the deploy section used for in WAML?\",\n",
    "    \"How do you specify the owner of a repository in WAML?\",\n",
    "    \"What are external links used for in WAML?\",\n",
    "]\n",
    "\n",
    "print(\"Creating evaluation samples using the complete RAG pipeline...\")\n",
    "evaluation_samples = []\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"Generating response for query {i + 1}: {query[:50]}...\")\n",
    "\n",
    "    # Generate actual responses using the RAG system\n",
    "    sample = create_evaluation_sample_with_rag(\n",
    "        user_input=query,\n",
    "        vector_store=vector_store,\n",
    "        genai_client=genai_client,\n",
    "        collection_name=\"weave_docs\",\n",
    "        top_k=VECTOR_TOP_K,\n",
    "        system_prompt_version=\"v1\",\n",
    "    )\n",
    "\n",
    "    evaluation_samples.append(sample)\n",
    "    print(f\"✓ Generated response: {sample.response[:100]}...\")\n",
    "\n",
    "print(\n",
    "    f\"\\nCreated {len(evaluation_samples)} evaluation samples with REAL RAG responses!\"\n",
    ")\n",
    "print(\"Each sample contains:\")\n",
    "print(\"  - User query\")\n",
    "print(\"  - RAG-generated response (using app_201.py pipeline)\")\n",
    "print(\"  - Retrieved contexts from vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = await batch_evaluate_samples(evaluation_samples, context_precision)\n",
    "\n",
    "# # Calculate and display statistics\n",
    "import statistics\n",
    "\n",
    "avg_score = statistics.mean(scores)\n",
    "print(f\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Average Context Precision: {avg_score:.3f}\")\n",
    "print(f\"Min Score: {min(scores):.3f}\")\n",
    "print(f\"Max Score: {max(scores):.3f}\")\n",
    "print(f\"Standard Deviation: {statistics.stdev(scores):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave-chatbot-reference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
